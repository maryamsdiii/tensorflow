# -*- coding: utf-8 -*-
"""tensorflow_HW1_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uoqjRG9kSGiJT04kM4gJjUyekXaAs-F_

# ===================================================================
Name   : Maryam Sadeghi

Std_ID : 99205286
# ===================================================================
"""

# mount to google drive
from google.colab import drive
drive.mount('/content/drive')

"""# 1. import library"""

import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import math
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from numpy.random import RandomState
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from PIL import Image
import csv
import cv2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer

# Commented out IPython magic to ensure Python compatibility.
# load tensorboard for the Histogram of Weights and ....
# %load_ext tensorboard

# convert label to alphabet
def convert(label):
  if label == '0':
    Alph = 'A'
  elif label == '1':
    Alph = 'B' 
  elif label == '2':
    Alph = 'C'
  elif label == '3':
    Alph = 'D'
  elif label == '4':
    Alph = 'E'
  elif label == '5':
    Alph = 'F'
  elif label == '6':
    Alph = 'G' 
  elif label == '7':
    Alph = 'H'
  elif label == '8':
    Alph = 'I'
  elif label == '9':
    Alph = 'J'
  elif label == '10':
    Alph = 'K'
  elif label == '11':
    Alph = 'L'
  elif label == '12':
    Alph = 'M'
  elif label == '13':
    Alph = 'N'
  elif label == '14':
    Alph = 'O'
  elif label == '15':
    Alph = 'P'
  elif label == '16':
    Alph = 'Q'
  elif label == '17':
    Alph = 'R'
  elif label == '18':
    Alph = 'S'
  elif label == '19':
    Alph = 'T'
  elif label == '20':
    Alph = 'U'
  elif label == '21':
    Alph = 'V'
  elif label == '22':
    Alph = 'W'
  elif label == '23':
    Alph = 'X'
  elif label == '24':
    Alph = 'Y'

  return Alph;

# plot samples of each Class
with open('/content/drive/MyDrive/Q1_train.csv') as csv_file:
    csv_reader = csv.reader(csv_file)

    # skip headers
    next(csv_reader)

    count=1
    for row in csv.reader(csv_file):
      if (count>25):
        break
      pixels = row[1:]                                           # without label
      pixels = np.array(pixels, dtype='uint8')
      pixels = pixels.reshape((28, 28))                          # reshape
      image = Image.fromarray(pixels)
      label = row[0]                                             # label
      Alph = convert(label)
      plt.figure()
      plt.imshow(image)
      plt.title(Alph)
      count+=1

"""# 2. Read Dataset"""

df1 = pd.read_csv('/content/drive/MyDrive/Q1_train.csv')
df2 = pd.read_csv('/content/drive/MyDrive/Q1_test.csv')

RS = RandomState()

# Split Dataset to Train & Valid Part 
train = df1.sample(frac=0.7, random_state = RS)            
valid = df1.loc[~df1.index.isin(train.index)]

# Get Number of Valid & Train Dataset
num_valid = valid.shape[0]
num_train = train.shape[0]

# Split input & Output of Tast Dataset
x_test = df2.iloc[:,1:]
x_test = np.array(x_test, dtype = 'uint32')/255
y_test = df2.iloc[:,0]

# Split input & Output of Train Dataset
y_train = train.iloc[:,0]
x_train = train.iloc[:,1:]
x_train = np.array(x_train, dtype='uint32')/255

# Split input & Output of Valid Dataset
y_valid  = valid.iloc[:,0]
x_valid  = valid.iloc[:,1:]
x_valid  = np.array(x_valid, dtype='uint32')/255

# Convert Lables to Onehot Vector
y_train_hot = tf.Session().run(tf.one_hot(y_train,25))
y_valid_hot = tf.Session().run(tf.one_hot(y_valid,25))
y_test_hot  = tf.Session().run(tf.one_hot(y_test,25))

# Close the session
tf.Session().close()

"""# 3. Shuffle"""

x_train_shaffle = tf.Session().run(tf.random.shuffle(x_train, seed = 1000 ))
tf.Session().close()

y_train_shaffle = tf.Session().run(tf.random.shuffle(y_train_hot, seed=1000))
tf.Session().close()

"""# 4. Choose your Hyper Parameters"""

num_feature = x_train.shape[1]
num_class   = 25
rl          = 0.001
batchsize   = 100
normal_std  = 0.001

"""# 5. Functions that we'll need"""

# Define Weights of your NN
def WEIGHTS(shape, name) :
    return tf.get_variable(dtype = tf.float32, shape = shape,name= 'W_' + name,
                  initializer=tf.truncated_normal_initializer(stddev = 0.001))

# Define Bias of your NN
def BIAS (shape, name):
    return tf.get_variable(dtype=tf.float32, shape = (shape) , name = 'B_'+name,
                  initializer = tf.zeros_initializer())

#Design your NN
def FC(X, num_units, name, relu = False):

    in_dim = X.get_shape()[1]
    W = WEIGHTS((in_dim, num_units), name)
    
    B = BIAS ((num_units), name)
    H = tf.matmul(X,W)
    O = tf.add(H,B)
    if relu == True:
        return tf.nn.relu(O),W
    return O,W

"""# 6. Neural Network ( Graph Phase )"""

tf.reset_default_graph()

input_image   = tf.placeholder(dtype = tf.float32, shape = (None,num_feature),
                           name = 'INPUT')


FC1 , weight1 = FC(input_image, 512, 'FC1', relu = True)

H2  , weight2   = FC(FC1, 25, 'FC2', relu = False)

output_image  = tf.placeholder(dtype = tf.float32, shape = (None, num_class),
                           name = 'OUTPUT')

"""# 7. Neural Network ( Graph Phase ) - Optimization """

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = output_image, logits = H2,
                                                              name = 'LOSS'))

# Adam Optimizer                                           
adam = tf.train.AdamOptimizer (
    learning_rate = rl, name = 'Adam_Optimizer'
).minimize(loss)

# SGD Optmizer
sgd = tf.train.GradientDescentOptimizer (
    learning_rate = rl, name = 'SGD_Optmizer'
).minimize(loss)

# Accuracy
accuracy      = tf.reduce_mean(tf.cast(tf.math.equal(tf.argmax(H2,axis=1),tf.argmax(output_image,axis=1)),dtype = tf.float32))
find_misclass = tf.where(tf.math.logical_not(tf.math.equal(tf.argmax(H2,axis = 1),tf.argmax(output_image,axis = 1))))

"""# 8. Neural Network ( Session Phase )  with adam optimizer"""

# Loss Summary 
loss_tensor = tf.summary.scalar('LOSS_TENSORBOARD' ,loss)

# Accuracy Summary
accuracy_tensor = tf.summary.scalar('Accuracy_TENSORBOARD' ,accuracy)

# Weights Histogram
weights_1 = tf.summary.histogram ('histogram_of_weights1', weight1)
weights_2 = tf.summary.histogram ('histogram_of_weights2', weight2)

# plot valid loss and accuracy
loss_valid_plt      = []
accuracy_valid_plt  = []

# plot train loss and accuracy
loss_train_plt      = []
accuracy_train_plt  = []

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())


write = tf.summary.FileWriter('./graphs', sess.graph)

for epoch in range(25):

    print('==================================================')
    print('=================== Epoch = %s ===================' %(epoch))
    print('==================================================')

    for i in range(math.floor(num_train/batchsize)):
        
        # Set Placehold Parameters
        f = {input_image :x_train_shaffle[i*batchsize : (i+1) * batchsize] , 
             output_image:y_train_shaffle[i*batchsize : (i+1) * batchsize] }
        
        sess.run(adam,feed_dict = f)

        # Write Summary
        write.add_summary(sess.run(loss_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(accuracy_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_1,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_2,feed_dict=f),epoch)
        
        
        if i%100 == 0:
          print('iter {} : \t Loss = {:.2f} \t training accuracy = {:.2f} '.format(i,sess.run(loss,feed_dict = f),sess.run(accuracy,feed_dict=f)))
  
    loss_train_plt.append(sess.run(loss,feed_dict=f))
    accuracy_train_plt.append(sess.run(accuracy,feed_dict=f))

    f_valid = {input_image:x_valid , 
              output_image:y_valid_hot}
    
    

    print('--------------------------------------------------')
    print('Epoch : {} \t Validation Loss : {:.2f} \t validation accuracy = {:.2f}'.format(epoch,sess.run(loss,feed_dict=f_valid),sess.run(accuracy,feed_dict=f_valid)))
    print('--------------------------------------------------')
    loss_valid_plt.append(sess.run(loss,feed_dict=f_valid))
    accuracy_valid_plt.append(sess.run(accuracy,feed_dict=f_valid))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./graphs

# test your network :)

f = {input_image :x_test , 
     output_image:y_test_hot}

print("Loss of Adam Optimizer : " , sess.run(loss,feed_dict=f))
print("Accuracy of Adam Optimizer : " ,sess.run(accuracy,feed_dict=f))

prediction = tf.argmax(tf.nn.softmax(H2),1)
tested = prediction.eval(feed_dict = {input_image: x_test}, session=sess)
print ("predictions : ", tested)

print(np.equal(tested,y_test))

# plot the results 
plt.plot(range(epoch + 1),loss_valid_plt,label = 'validation loss', color = 'y',LineWidth = 3)
plt.plot(range(epoch + 1),loss_train_plt,label = 'train loss', color = 'k',LineWidth = 3)
plt.title("Loss variation with adam optimizer")
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
loss_adam_1 = loss_valid_plt

plt.plot(range(epoch + 1),accuracy_valid_plt,label = 'validation accuracy', color = 'y',LineWidth=3)
plt.plot(range(epoch + 1),accuracy_train_plt,label = 'train accuracy', color = 'k',LineWidth=3)
plt.title("accuracy variation with adam optimizer")
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
accuracy_adam_1 = accuracy_valid_plt

# close the session
sess.close()

"""# 9. Neural Network ( Session Phase )  with gradient descent optimizer"""

# Loss Summary 
loss_tensor = tf.summary.scalar('LOSS_TENSORBOARD' ,loss)

# Accuracy Summary
accuracy_tensor = tf.summary.scalar('Accuracy_TENSORBOARD' ,accuracy)

# Weights Histogram
weights_1 = tf.summary.histogram ('histogram_of_weights1', weight1)
weights_2 = tf.summary.histogram ('histogram_of_weights2', weight2)

# plot valid loss and accuracy
loss_valid_plt      = []
accuracy_valid_plt  = []

# plot train loss and accuracy
loss_train_plt      = []
accuracy_train_plt  = []

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())


write = tf.summary.FileWriter('./graphs', sess.graph)

for epoch in range(25):

    print('==================================================')
    print('=================== Epoch = %s ===================' %(epoch))
    print('==================================================')

    for i in range(math.floor(num_train/batchsize)):
        
        # Set Placehold Parameters
        f = {input_image :x_train_shaffle[i*batchsize : (i+1) * batchsize] , 
             output_image:y_train_shaffle[i*batchsize : (i+1) * batchsize] }
        
        sess.run(sgd,feed_dict = f)

        # Write Summary
        write.add_summary(sess.run(loss_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(accuracy_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_1,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_2,feed_dict=f),epoch)
        
        
        if i%100 == 0:
          print('iter {} : \t Loss = {:.2f} \t training accuracy = {:.2f} '.format(i,sess.run(loss,feed_dict = f),sess.run(accuracy,feed_dict=f)))
  
    loss_train_plt.append(sess.run(loss,feed_dict=f))
    accuracy_train_plt.append(sess.run(accuracy,feed_dict=f))

    f_valid = {input_image:x_valid , 
              output_image:y_valid_hot}
    
    

    print('--------------------------------------------------')
    print('Epoch : {} \t Validation Loss : {:.2f} \t validation accuracy = {:.2f}'.format(epoch,sess.run(loss,feed_dict=f_valid),sess.run(accuracy,feed_dict=f_valid)))
    print('--------------------------------------------------')
    loss_valid_plt.append(sess.run(loss,feed_dict=f_valid))
    accuracy_valid_plt.append(sess.run(accuracy,feed_dict=f_valid))

# test your network :)

f = {input_image :x_test , 
     output_image:y_test_hot}

print("Loss of Test Dataset : " ,sess.run(loss,feed_dict=f))
print("Loss of Test Dataset : " ,sess.run(accuracy,feed_dict=f))

prediction = tf.argmax(tf.nn.softmax(H2),1)
tested = prediction.eval(feed_dict = {input_image: x_test}, session=sess)
print ("predictions : ", tested)

print(np.equal(tested,y_test))

# plot the results 
plt.plot(range(epoch + 1),loss_valid_plt,label = 'validation loss', color = 'y',LineWidth = 3)
plt.plot(range(epoch + 1),loss_train_plt,label = 'train loss', color = 'k',LineWidth = 3)
plt.title("Loss variation with SGD optimizer")
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
loss_sgd_1 = loss_valid_plt

plt.plot(range(epoch + 1),accuracy_valid_plt,label = 'validation accuracy', color = 'y',LineWidth=3)
plt.plot(range(epoch + 1),accuracy_train_plt,label = 'train accuracy', color = 'k',LineWidth=3)
plt.title("accuracy variation with SGD optimizer")
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
accuracy_sgd_1 = accuracy_valid_plt

plt.plot(range(epoch + 1),accuracy_sgd_1,label  = 'validation accuracy for SGD', color = 'y',LineWidth=3)
plt.plot(range(epoch + 1),accuracy_adam_1,label = 'validation accuracy for ADAM', color = 'k',LineWidth=3)

plt.plot(range(epoch + 1),loss_sgd_1,label  = 'validation loss for SGD', color = 'y',LineWidth=3)
plt.plot(range(epoch + 1),loss_adam_1,label = 'validation loss for ADAM', color = 'k',LineWidth=3)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./graphs

# close the session
sess.close()

"""# 10. Add Dropout layer

"""

tf.reset_default_graph()
input_image = tf.placeholder(dtype=tf.float32, shape=(None,num_feature),
                           name='INPUT')

keep = tf.placeholder(dtype=tf.float32)

FC1 , weight1 = FC(input_image, 512, 'FC1', relu = True)
o1 = tf.nn.dropout(FC1, keep_prob = keep)

H2 , weight2  = FC(o1, 25, 'FC2', relu = False)

output_image = tf.placeholder(dtype=tf.float32, shape = (None, num_class),
                           name = 'OUTPUT')

# same as the previous ... :)

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = output_image, logits = H2,
                                            name='LOSS'))
adam = tf.train.AdamOptimizer (
    learning_rate=rl, name = 'Adam'
).minimize(loss)

accuracy = tf.reduce_mean(tf.cast(tf.math.equal(tf.argmax(H2,axis=1),tf.argmax(output_image,axis=1)),dtype=tf.float32))
find_misclass = tf.where(tf.math.logical_not(tf.math.equal(tf.argmax(H2,axis=1),tf.argmax(output_image,axis=1))))

loss_tensorboard= tf.summary.scalar('LOSS_TENSORBOARD' ,loss)
accuracy_tensor= tf.summary.scalar('accuracy_TENSORBOARD' ,accuracy)

# Loss Summary 
loss_tensor = tf.summary.scalar('LOSS_TENSORBOARD' ,loss)

# Accuracy Summary
accuracy_tensor = tf.summary.scalar('Accuracy_TENSORBOARD' ,accuracy)

# Weights Histogram
weights_1 = tf.summary.histogram ('histogram_of_weights1', weight1)
weights_2 = tf.summary.histogram ('histogram_of_weights2', weight2)

# plot valid loss and accuracy
loss_valid_plt      = []
accuracy_valid_plt  = []

# plot train loss and accuracy
loss_train_plt      = []
accuracy_train_plt  = []

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())


write = tf.summary.FileWriter('./graphs', sess.graph)

for epoch in range(30):

    print('==================================================')
    print('=================== Epoch = %s ===================' %(epoch))
    print('==================================================')

    for i in range(math.floor(num_train/batchsize)):
        
        # Set Placehold Parameters
        f = {input_image :x_train_shaffle[i*batchsize : (i+1) * batchsize] , 
             output_image:y_train_shaffle[i*batchsize : (i+1) * batchsize] ,
             keep : 0.5}
        
        sess.run(adam,feed_dict = f)

        # Write Summary
        write.add_summary(sess.run(loss_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(accuracy_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_1,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_2,feed_dict=f),epoch)
        
        
        if i%100 == 0:
          print('iter {} : \t Loss = {:.2f} \t training accuracy = {:.2f} '.format(i,sess.run(loss,feed_dict = f),sess.run(accuracy,feed_dict=f)))
  
    loss_train_plt.append(sess.run(loss,feed_dict=f))
    accuracy_train_plt.append(sess.run(accuracy,feed_dict=f))

    f_valid = {input_image:x_valid , 
              output_image:y_valid_hot,
               keep : 0.5}
    
    

    print('--------------------------------------------------')
    print('Epoch : {} \t Validation Loss : {:.2f} \t validation accuracy = {:.2f}'.format(epoch,sess.run(loss,feed_dict=f_valid),sess.run(accuracy,feed_dict=f_valid)))
    print('--------------------------------------------------')
    loss_valid_plt.append(sess.run(loss,feed_dict=f_valid))
    accuracy_valid_plt.append(sess.run(accuracy,feed_dict=f_valid))

# test your network :)

f = {input_image :x_test , 
     output_image:y_test_hot,
     keep : 0.5}

print("Loss of Dropout : " ,sess.run(loss,feed_dict=f))
print("Accuracy of Dropout : " ,sess.run(accuracy,feed_dict=f))

prediction = tf.argmax(tf.nn.softmax(H2),1)
tested = prediction.eval(feed_dict = {input_image: x_test,keep:0.5}, session=sess)
print ("predictions : ", tested)

print(np.equal(tested,y_test))

# plot the results 
plt.plot(range(epoch + 1),loss_valid_plt,label = 'validation loss', color = 'y',LineWidth = 3)
plt.plot(range(epoch + 1),loss_train_plt,label = 'train loss', color = 'k',LineWidth = 3)
plt.title("Loss variation with Adam optimizer")
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
loss_droupout = loss_valid_plt

# plot the results 
plt.plot(range(epoch + 1),accuracy_valid_plt,label = 'validation Accuracy', color = 'y',LineWidth = 3)
plt.plot(range(epoch + 1),accuracy_train_plt,label = 'train Accuracy', color = 'k',LineWidth = 3)
plt.title("Loss variation with Adam optimizer ")
plt.xlabel('epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
accuracy_droupout = accuracy_valid_plt

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./graphs

# close the session
sess.close()

"""# 11. Add batch normalization layer

"""

tf.reset_default_graph()
input_image = tf.placeholder(dtype=tf.float32, shape=(None,num_feature),
                           name='INPUT')

keep = tf.placeholder(dtype=tf.float32)
scale1=tf.Variable(tf.ones([512]))
beta1=tf.Variable(tf.zeros([512]))

FC1 , weight1  = FC(input_image, 512, 'FC1', relu=True)

inputs_shape = FC1.get_shape()
inputs_rank = inputs_shape.ndims
axis = range(1, inputs_rank)
mean_x_1, std_x_1 = tf.nn.moments(FC1, axis, keep_dims=True)
bn1 = tf.nn.batch_normalization(FC1,mean_x_1, std_x_1, beta1, scale1, 1e-12)

scale2=tf.Variable(tf.ones([256]))
beta2=tf.Variable(tf.zeros([256]))

FC2 , weight2 = FC(bn1, 256, 'FC2', relu = True)

inputs_shape = FC2.get_shape()
inputs_rank = inputs_shape.ndims
axis = range(1, inputs_rank)
mean_x_2, std_x_2 = tf.nn.moments(FC2, axis, keep_dims=True)
bn2 = tf.nn.batch_normalization(FC2,mean_x_2, std_x_2 , beta2, scale2, 1e-12)


scale3=tf.Variable(tf.ones([128]))
beta3=tf.Variable(tf.zeros([128]))

FC3 , weight3 = FC(bn2, 128, 'FC3', relu=True)

inputs_shape = FC3.get_shape()
inputs_rank = inputs_shape.ndims
axis = range(1, inputs_rank)
mean_x_3, std_x_3 = tf.nn.moments(FC3, axis , keep_dims=True)
bn3 = tf.nn.batch_normalization(FC3,mean_x_3, std_x_3, beta3, scale3, 1e-12)

H2 , weight4 = FC(bn3, 25, 'FC7', relu=False)

output_image = tf.placeholder(dtype=tf.float32, shape=(None, num_class),
                           name='OUTPUT')

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=output_image, logits= H2,
                                            name='LOSS'))
adam=tf.train.AdamOptimizer (
    learning_rate=rl, name='Adam'
).minimize(loss)


accuracy = tf.reduce_mean(tf.cast(tf.math.equal(tf.argmax(H2,axis=1),tf.argmax(output_image,axis=1)),dtype=tf.float32))
find_misclass = tf.where(tf.math.logical_not(tf.math.equal(tf.argmax(H2,axis=1),tf.argmax(output_image,axis=1))))

loss_tensor = tf.summary.scalar('LOSS_TENSORBOARD' ,loss)
accuracy_tensor = tf.summary.scalar('accuracy_TENSORBOARD' ,accuracy)

# Loss Summary 
loss_tensor = tf.summary.scalar('LOSS_TENSORBOARD' ,loss)

# Accuracy Summary
accuracy_tensor = tf.summary.scalar('Accuracy_TENSORBOARD' ,accuracy)

# Weights Histogram
weights_1 = tf.summary.histogram ('histogram_of_weights1', weight1)
weights_2 = tf.summary.histogram ('histogram_of_weights2', weight2)
weights_3 = tf.summary.histogram ('histogram_of_weights3', weight3)
weights_4 = tf.summary.histogram ('histogram_of_weights4', weight4)
#weights_5 = tf.summary.histogram ('histogram_of_weights5', weight5)
#weights_6 = tf.summary.histogram ('histogram_of_weights6', weight6)
#weights_7 = tf.summary.histogram ('histogram_of_weights7', weight7)

# plot valid loss and accuracy
loss_valid_plt      = []
accuracy_valid_plt  = []

# plot train loss and accuracy
loss_train_plt      = []
accuracy_train_plt  = []

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())


write = tf.summary.FileWriter('./graphs', sess.graph)

for epoch in range(16):

    print('==================================================')
    print('=================== Epoch = %s ===================' %(epoch))
    print('==================================================')

    for i in range(math.floor(num_train/batchsize)):
        
        # Set Placehold Parameters
        f = {input_image :x_train_shaffle[i*batchsize : (i+1) * batchsize] , 
             output_image:y_train_shaffle[i*batchsize : (i+1) * batchsize] ,
             keep : 0.5}
        
        sess.run(adam,feed_dict = f)

        # Write Summary
        write.add_summary(sess.run(loss_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(accuracy_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_1,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_2,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_3,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_4,feed_dict=f),epoch)
        #write.add_summary(sess.run(weights_5,feed_dict=f),epoch)
        #write.add_summary(sess.run(weights_6,feed_dict=f),epoch)
        #write.add_summary(sess.run(weights_7,feed_dict=f),epoch)
        
        
        if i%100 == 0:
          print('iter {} : \t Loss = {:.2f} \t training accuracy = {:.2f} '.format(i,sess.run(loss,feed_dict = f),sess.run(accuracy,feed_dict=f)))
  
    loss_train_plt.append(sess.run(loss,feed_dict=f))
    accuracy_train_plt.append(sess.run(accuracy,feed_dict=f))

    f_valid = {input_image:x_valid , 
              output_image:y_valid_hot,
               keep : 0.5}
    
    

    print('--------------------------------------------------')
    print('Epoch : {} \t Validation Loss : {:.2f} \t validation accuracy = {:.2f}'.format(epoch,sess.run(loss,feed_dict=f_valid),sess.run(accuracy,feed_dict=f_valid)))
    print('--------------------------------------------------')
    loss_valid_plt.append(sess.run(loss,feed_dict=f_valid))
    accuracy_valid_plt.append(sess.run(accuracy,feed_dict=f_valid))

# test your network :)

f = {input_image :x_test , 
     output_image:y_test_hot}

print("Loss of Test Dataset : " , sess.run(loss,feed_dict=f))
print("Accuracy of Test Dataset : " ,sess.run(accuracy,feed_dict=f))

prediction = tf.argmax(tf.nn.softmax(H2),1)
tested = prediction.eval(feed_dict = {input_image: x_test}, session=sess)
print ("predictions : ", tested)

print(np.equal(tested,y_test))

# plot Confusion Matrix
confusionMatrixTest = tf.confusion_matrix(labels = y_test, predictions = tested)
cm = sess.run(confusionMatrixTest)

print(confusionMatrixTest.shape)

plt.figure(figsize = (15,15))
ax = sns.heatmap(cm, annot = True, cmap = "RdBu")
plt.show()

# find the best tested class
maxx = np.unravel_index(np.argmax(cm, axis=None), cm.shape) 
maxx = maxx[0]
print(" The best class is : " + str(maxx))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./graphs

# plot the results 
plt.plot(range(epoch + 1),loss_valid_plt,label = 'validation loss', color = 'y',LineWidth = 3)
plt.plot(range(epoch + 1),loss_train_plt,label = 'train loss', color = 'k',LineWidth = 3)
plt.title("Loss variation with َ Adam optimizer Batch Normalization")
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
loss_batchNormal = loss_valid_plt

# plot the results 
plt.plot(range(epoch + 1),accuracy_valid_plt,label = 'validation loss', color = 'y',LineWidth = 3)
plt.plot(range(epoch + 1),accuracy_train_plt,label = 'train loss', color = 'k',LineWidth = 3)
plt.title("Accuracy variation with Adam optimizer Batch Normalization")
plt.xlabel('epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
accuracy_batchNormal = accuracy_valid_plt

plt.plot(accuracy_adam_1, label = 'validation Accuracy',color = 'k',LineWidth=3)
plt.plot(accuracy_droupout,label = 'validation Accuracy Dropout',color = 'y',LineWidth=3)
plt.plot(accuracy_batchNormal,label = 'validation Accuracy BatchNormal',color = 'r',LineWidth=3)
plt.xlabel('epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()

"""## Down Sampling"""

cnt=0
list_max=[]
df = np.array(df1)
for i in range(df1.shape[0]):
  if df[i,0] == maxx:
    list_max.append(i)
    cnt+=1

cnt=4*cnt/5

print(list_max)
print(len(list_max))

list_max_remove=[]
# Down sampling
uniform_sample = set(np.random.randint(len(list_max), size=1800)) #generate some more for the duplicates

print(len(uniform_sample))
uniform_sample = list(uniform_sample)
for i in range(math.floor(cnt)):
  list_max_remove.append(list_max[uniform_sample[i]])

print(len(list_max_remove))

print(len(uniform_sample))

df1=df1.drop(list_max_remove)

print(df1.shape[0])

sess.close()

df2 = pd.read_csv('/content/drive/MyDrive/Q1_test.csv')

RS = RandomState()

# Split Dataset to Train & Valid Part 
train = df1.sample(frac=0.7, random_state = RS)            
valid = df1.loc[~df1.index.isin(train.index)]

# Get Number of Valid & Train Dataset
num_valid = valid.shape[0]
num_train = train.shape[0]

# Split input & Output of Tast Dataset
x_test = df2.iloc[:,1:]
x_test = np.array(x_test, dtype = 'uint32')/255
y_test = df2.iloc[:,0]

# Split input & Output of Train Dataset
y_train = train.iloc[:,0]
x_train = train.iloc[:,1:]
x_train = np.array(x_train, dtype='uint32')/255

# Split input & Output of Valid Dataset
y_valid  = valid.iloc[:,0]
x_valid  = valid.iloc[:,1:]
x_valid  = np.array(x_valid, dtype='uint32')/255

# Convert Lables to Onehot Vector
y_train_hot = tf.Session().run(tf.one_hot(y_train,25))
y_valid_hot = tf.Session().run(tf.one_hot(y_valid,25))
y_test_hot  = tf.Session().run(tf.one_hot(y_test,25))

# Close the session
tf.Session().close()

# Define Weights of your NN
def WEIGHTS(shape, name) :
    return tf.get_variable(dtype = tf.float32, shape = shape,name= 'W_' + name,
                  initializer=tf.truncated_normal_initializer(stddev = 0.001))

# Define Bias of your NN
def BIAS (shape, name):
    return tf.get_variable(dtype=tf.float32, shape = (shape) , name = 'B_'+name,
                  initializer = tf.zeros_initializer())

# Design your NN
def FC(X, num_units, name, relu = False):

    in_dim = X.get_shape()[1]
    W = WEIGHTS((in_dim, num_units), name)
    
    B = BIAS ((num_units), name)
    H = tf.matmul(X,W)
    O = tf.add(H,B)
    if relu == True:
        return tf.nn.relu(O),W
    return O,W

tf.reset_default_graph()

input_image   = tf.placeholder(dtype = tf.float32, shape = (None,num_feature),
                           name = 'INPUT')


FC1 , weight1 = FC(input_image, 512, 'FC1', relu = True)

H2  , weight2   = FC(FC1, 25, 'FC2', relu = False)

output_image  = tf.placeholder(dtype = tf.float32, shape = (None, num_class),
                           name = 'OUTPUT')

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = output_image, logits = H2,
                                                              name = 'LOSS'))

# Adam Optimizer                                           
adam = tf.train.AdamOptimizer (
    learning_rate = rl, name = 'Adam_Optimizer'
).minimize(loss)

# Accuracy
accuracy      = tf.reduce_mean(tf.cast(tf.math.equal(tf.argmax(H2,axis=1),tf.argmax(output_image,axis=1)),dtype = tf.float32))
find_misclass = tf.where(tf.math.logical_not(tf.math.equal(tf.argmax(H2,axis = 1),tf.argmax(output_image,axis = 1))))

# Loss Summary 
loss_tensor = tf.summary.scalar('LOSS_TENSORBOARD' ,loss)

# Accuracy Summary
accuracy_tensor = tf.summary.scalar('Accuracy_TENSORBOARD' ,accuracy)

# Weights Histogram
weights_1 = tf.summary.histogram ('histogram_of_weights1', weight1)
weights_2 = tf.summary.histogram ('histogram_of_weights2', weight2)

# plot valid loss and accuracy
loss_valid_plt      = []
accuracy_valid_plt  = []

# plot train loss and accuracy
loss_train_plt      = []
accuracy_train_plt  = []

sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())


write = tf.summary.FileWriter('./graphs', sess.graph)

for epoch in range(25):

    print('==================================================')
    print('=================== Epoch = %s ===================' %(epoch))
    print('==================================================')

    for i in range(math.floor(num_train/batchsize)):
        
        # Set Placehold Parameters
        f = {input_image :x_train_shaffle[i*batchsize : (i+1) * batchsize] , 
             output_image:y_train_shaffle[i*batchsize : (i+1) * batchsize] }
        
        sess.run(adam,feed_dict = f)

        # Write Summary
        write.add_summary(sess.run(loss_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(accuracy_tensor,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_1,feed_dict=f),epoch)
        write.add_summary(sess.run(weights_2,feed_dict=f),epoch)
        
        
        if i%100 == 0:
          print('iter {} : \t Loss = {:.2f} \t training accuracy = {:.2f} '.format(i,sess.run(loss,feed_dict = f),sess.run(accuracy,feed_dict=f)))
  
    loss_train_plt.append(sess.run(loss,feed_dict=f))
    accuracy_train_plt.append(sess.run(accuracy,feed_dict=f))

    f_valid = {input_image:x_valid , 
              output_image:y_valid_hot}
    
    

    print('--------------------------------------------------')
    print('Epoch : {} \t Validation Loss : {:.2f} \t validation accuracy = {:.2f}'.format(epoch,sess.run(loss,feed_dict=f_valid),sess.run(accuracy,feed_dict=f_valid)))
    print('--------------------------------------------------')
    loss_valid_plt.append(sess.run(loss,feed_dict=f_valid))
    accuracy_valid_plt.append(sess.run(accuracy,feed_dict=f_valid))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./graphs

# test your network :)

f = {input_image :x_test , 
     output_image:y_test_hot}

print("Loss of Test Dataset : " , sess.run(loss,feed_dict=f))
print("Accuracy of Test Dataset : " ,sess.run(accuracy,feed_dict=f))

prediction = tf.argmax(tf.nn.softmax(H2),1)
tested = prediction.eval(feed_dict = {input_image: x_test}, session=sess)
print ("predictions : ", tested)

print(np.equal(tested,y_test))

print(np.equal(tested,y_test))

# plot Confusion Matrix
confusionMatrixTest = tf.confusion_matrix(labels = y_test, predictions = tested)
cm = sess.run(confusionMatrixTest)

print(confusionMatrixTest.shape)

plt.figure(figsize = (15,15))
ax = sns.heatmap(cm, annot = True, cmap = "RdBu")
plt.show()

# plot the results 
plt.plot(range(epoch + 1),loss_valid_plt,label = 'validation loss', color = 'y',LineWidth = 3)
plt.plot(range(epoch + 1),loss_train_plt,label = 'train loss', color = 'k',LineWidth = 3)
plt.title("Loss variation with َ Adam optimizer with Down Sampling")
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
loss_downsamoling = loss_valid_plt

# plot the results 
plt.plot(range(epoch + 1),accuracy_valid_plt,label = 'validation loss', color = 'y',LineWidth = 3)
plt.plot(range(epoch + 1),accuracy_train_plt,label = 'train loss', color = 'k',LineWidth = 3)
plt.title("Accuracy variation with Adam optimizer with Down Sampling")
plt.xlabel('epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()

# saving the results ...
accuracy_downsamoling = accuracy_valid_plt

# copmare results
plt.plot(accuracy_downsamoling,label = 'accuracy_downsamoling', color = 'y',LineWidth = 3)
plt.plot(accuracy_batchNormal,label = 'accuracy_batchNormal', color = 'k',LineWidth = 3)
plt.plot(accuracy_droupout,label = 'accuracy_droupout', color = 'k',LineWidth = 3)
plt.plot(accuracy_adam_1,label = 'accuracy_adam_1', color = 'k',LineWidth = 3)
plt.xlabel('epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()

plt.plot(loss_downsamoling,label = 'loss_downsamoling', color = 'y',LineWidth = 3)
plt.plot(loss_batchNormal,label = 'loss_batchNormal', color = 'k',LineWidth = 3)
plt.plot(loss_droupout,label = 'loss_droupout', color = 'k',LineWidth = 3)
plt.plot(loss_adam_1,label = 'loss_adam_1', color = 'k',LineWidth = 3)
plt.xlabel('epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()

"""# 12.  capture photo"""

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)
  data = eval_js('takePhoto({})'.format(quality))
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)
  return filename

from IPython.display import Image
for i in range(3):
  try:
    filename = take_photo(filename='photo'+str(i)+'.jpg')
    print('Saved to {}'.format(filename))
    
    # Show the image which was just taken.
    display(Image(filename))
  except Exception as err:
    # Errors will be thrown if the user does not have a webcam or if they do not
    # grant the page permission to access it.
    print(str(err))

from PIL import Image
for i in range(3):
  img = Image.open('photo'+str(i)+'.jpg')
  imgGray = img.convert('L')
  imgGray.save('test'+str(i)+'.jpg')
  display(imgGray)

import cv2
predict=[]
for i in range(3):
  img=cv2.imread('test'+str(i)+'.jpg',cv2.IMREAD_UNCHANGED)
  img= cv2.resize(img,(28,28))
  img = img.reshape((1,784))/255
  prediction=tf.argmax(tf.nn.softmax(H2),1)
  predict.append(prediction.eval(feed_dict={input_image: img}, session=sess))
print ("predictions :", predict)